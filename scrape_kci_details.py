import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import re

KCI_URL = 'https://www.kci.go.kr/kciportal/po/search/poArtiSearList.kci'
SEARCH_KEYWORD1 = 'ë¬¸í™”ìœ ì‚° OR "cultural heritage"'
SEARCH_KEYWORD2 = 'íë ˆì´ì…˜'

articles_data = []

async def extract_detail_info(detail_page):
    """
    ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì´ˆë¡, ì €ì_ìƒì„¸, í‚¤ì›Œë“œ_ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì œê³µëœ HTML ìŠ¤ë‹ˆí«ì„ ê¸°ë°˜ìœ¼ë¡œ ì…€ë ‰í„°ë¥¼ ì„¸ë°€í•˜ê²Œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.
    """
    abstract_text = ""
    authors_full_list = ""
    keywords_combined = ""

    # --- 1. ì´ˆë¡ ì¶”ì¶œ (êµ­ë¬¸/ì˜ë¬¸ ëª¨ë‘ í¬í•¨) ---
    try:
        # IDê°€ korAbst ë˜ëŠ” folaAbstì¸ ëª¨ë“  p íƒœê·¸ë¥¼ ì°¾ìŒ
        abstract_locators = detail_page.locator('p#korAbst, p#folaAbst') 
        
        extracted_abstracts = []
        for i in range(await abstract_locators.count()):
            content = await abstract_locators.nth(i).text_content()
            if content.strip(): 
                extracted_abstracts.append(content.strip())
        
        abstract_text = "\n".join(extracted_abstracts) # êµ­ë¬¸/ì˜ë¬¸ ì´ˆë¡ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—°ê²°
        if not abstract_text:
            print("    âš ï¸ ì´ˆë¡ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"    âŒ ì´ˆë¡ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    # --- 2. ì €ì ëª©ë¡ ì¶”ì¶œ (ì €ì ì¤‘ë³µ ì œê±° ë° ì •ì œ) ---
    try:
        all_authors = []
        # 'div.author' ë‚´ì— ìˆëŠ” ëª¨ë“  'a' íƒœê·¸ (ì €ì ë§í¬)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        # ì´ ì…€ë ‰í„°ëŠ” 'div.author'ê°€ ì €ì ê·¸ë£¹ì˜ ì»¨í…Œì´ë„ˆì´ë©°, ê·¸ ì•ˆì— ê° ì €ì ë§í¬ê°€ ìˆìŒì„ ê°€ì •í•©ë‹ˆë‹¤.
        author_links = detail_page.locator('div.author a') 
        
        if await author_links.count() > 0:
            for i in range(await author_links.count()):
                author_link = author_links.nth(i)
                author_name_raw = await author_link.text_content()
                
                # ë¶ˆí•„ìš”í•œ ê³µë°±, ì¤„ë°”ê¿ˆ, ì´ë¦„ ë’¤ì˜ ìˆ«ìë‚˜ ì´ë©”ì¼ íŒ¨í„´ì„ ì œê±°
                author_name_cleaned = re.sub(r'\s*\d+(\s*@\S+)?$', '', author_name_raw).strip() 
                
                # êµ­ë¬¸/ì˜ë¬¸ ì´ë¦„ì´ '/'ë¡œ êµ¬ë¶„ëœ ê²½ìš°, êµ­ë¬¸ ì´ë¦„ë§Œ ì·¨í•¨
                if '/' in author_name_cleaned:
                    author_name_cleaned = author_name_cleaned.split('/')[0].strip()

                if author_name_cleaned: # ë¹„ì–´ìˆì§€ ì•Šì€ ì´ë¦„ë§Œ ì¶”ê°€
                    all_authors.append(author_name_cleaned)
            
            # ì¶”ì¶œëœ ì €ì ëª©ë¡ì—ì„œ ì¤‘ë³µì„ ì œê±°í•˜ê³  ì•ŒíŒŒë²³/ê°€ë‚˜ë‹¤ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ë¬¸ìì—´ë¡œ í•©ì¹©ë‹ˆë‹¤.
            authors_full_list = "; ".join(sorted(list(set(all_authors)))) 
        else:
            print("    âš ï¸ ì €ì ì •ë³´ (div.author a)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"    âŒ ì €ì ëª©ë¡ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    # --- 3. í‚¤ì›Œë“œ ì¶”ì¶œ (ì˜ì–´ í‚¤ì›Œë“œ ë° í•œê¸€ í‚¤ì›Œë“œ ëª¨ë‘ í¬í•¨) ---
    try:
        english_keywords = []
        korean_keywords_text = ""
        
        # í‚¤ì›Œë“œ ì„¹ì…˜ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ê¸° ìœ„í•œ ì—¬ëŸ¬ ì…€ë ‰í„° ì‹œë„ (HTML êµ¬ì¡° ë‹¤ì–‘ì„±ì— ëŒ€ì‘)
        # `p` íƒœê·¸ ë‚´ì— 'í‚¤ì›Œë“œ' ë˜ëŠ” 'Keywords' í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” innerBoxë¥¼ ìš°ì„  ì°¾ê±°ë‚˜,
        # 'a#keywd'ë¥¼ ê°€ì§„ innerBoxë¥¼ ì°¾ìŒ
        keywords_section_locator = detail_page.locator(
            'div.innerBox.open:has(p:has-text("í‚¤ì›Œë“œ")), ' 
            'div.innerBox.open:has(p:has-text("Keywords")), '
            'div.innerBox.open:has(a#keywd)'
        )
        
        if await keywords_section_locator.count() > 0:
            target_keywords_section = keywords_section_locator.first

            # ì˜ì–´ í‚¤ì›Œë“œ (a#keywd) ì¶”ì¶œ
            english_keywd_locators = target_keywords_section.locator('a#keywd')
            for i in range(await english_keywd_locators.count()):
                keyword = await english_keywd_locators.nth(i).text_content()
                english_keywords.append(keyword.strip())
            
            # í•œê¸€ í‚¤ì›Œë“œ (<p> íƒœê·¸) ì¶”ì¶œ (ìœ ì—°í•˜ê²Œ)
            korean_keywd_p_locators = target_keywords_section.locator('p')
            for i in range(await korean_keywd_p_locators.count()):
                p_text = await korean_keywd_p_locators.nth(i).text_content()
                # í•œê¸€ í¬í•¨, ê¸¸ì´ 10ì ì´ìƒ, 'ì´ˆë¡/ì €ì'ì™€ ê°™ì€ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ê°€ ì—†ëŠ” í…ìŠ¤íŠ¸ í•„í„°ë§
                if re.search(r'[ê°€-í£]', p_text) and len(p_text) > 10 and \
                   not any(phrase in p_text for phrase in ['ì´ˆë¡', 'ì €ì', 'abstract', 'author', 'ë³¸ ë…¼ë¬¸ì€']): 
                    korean_keywords_text = p_text.strip()
                    break 
            
            # í•œê¸€ í‚¤ì›Œë“œ ì •ì œ: ì‰¼í‘œë¥¼ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ë³€í™˜ ë° ê³µë°± ì œê±°
            if korean_keywords_text:
                korean_keywords_text = korean_keywords_text.replace(' ', '').replace(',', ';')
                korean_keywords_text = ';'.join(filter(None, korean_keywords_text.split(';')))

            # ì˜ë¬¸ í‚¤ì›Œë“œì™€ êµ­ë¬¸ í‚¤ì›Œë“œë¥¼ ê²°í•©
            if english_keywords and korean_keywords_text:
                keywords_combined = "; ".join(english_keywords) + ";" + korean_keywords_text
            elif english_keywords:
                keywords_combined = "; ".join(english_keywords)
            elif korean_keywords_text:
                keywords_combined = korean_keywords_text
            
            # ìµœì¢… ì •ì œ: ì¤‘ë³µ ì„¸ë¯¸ì½œë¡  ì œê±° ë° ì–‘ìª½ ê³µë°± ì œê±°
            keywords_combined = ';'.join(filter(None, keywords_combined.split(';')))
            keywords_combined = re.sub(r';\s*;', ';', keywords_combined).strip()

        if not keywords_combined:
             print("    âš ï¸ í‚¤ì›Œë“œ ì„¹ì…˜ì„ ì°¾ì•˜ìœ¼ë‚˜ ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"    âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    return abstract_text, authors_full_list, keywords_combined

async def extract_page_articles(page, browser): 
    rows = page.locator('table.search-answer-tbl > tbody > tr')
    count = await rows.count()
    print(f"ğŸ“„ í˜ì´ì§€ ë‚´ ë…¼ë¬¸ ìˆ˜: {count}")

    for i in range(count):
        detail_page = None 
        try:
            row = rows.nth(i)
            # ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ë¡œ ì—°ê²°ë˜ëŠ” ë§í¬ ì°¾ê¸°
            article_link_locator = row.locator('a.subject')
            
            if await article_link_locator.count() == 0: # ëŒ€ì²´ ì…€ë ‰í„°
                 article_link_locator = row.locator('a[href*="ciSereArtiView"]')

            # ë§í¬ ì—˜ë¦¬ë¨¼íŠ¸ê°€ í™”ë©´ì— ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ í™•ì‹¤íˆ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ ì¦ê°€)
            await article_link_locator.first.wait_for(state='visible', timeout=20000) 

            if await article_link_locator.count() == 0:
                print(f"  âŒ [{i+1}] ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                continue

            article_link_el = article_link_locator.first
            article_url_path = await article_link_el.get_attribute('href')
            article_title_on_search_page = await article_link_el.text_content()

            # --- ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ---
            inputs = row.locator("input[type='hidden']")
            input_count = await inputs.count()
            data_dict = {}
            for j in range(input_count):
                input_el = inputs.nth(j)
                name = await input_el.get_attribute('name')
                value = await input_el.get_attribute('value')
                data_dict[name] = value

            temp_article_data = {
                'ì œëª©': data_dict.get('R_INDE_TITL', '') or article_title_on_search_page.strip(),
                'ì €ë„ëª…': data_dict.get('R_SERE_NM', ''),
                'ë°œí–‰ê¸°ê´€': data_dict.get('R_PUBI_INSI_NM', ''),
                'ê¶Œ': data_dict.get('R_VOL', ''),
                'í˜¸': data_dict.get('R_ISSE', ''),
                'ì‹œì‘í˜ì´ì§€': data_dict.get('R_ST_PG', ''),
                'ì¢…ë£Œí˜ì´ì§€': data_dict.get('R_END_PG', ''),
                'ë°œí–‰ë…„ë„': data_dict.get('R_PUBI_DT', '')[:4],
                'ì£¼ì œë¶„ì•¼': data_dict.get('R_MAJOR', ''),
                'ì¸ìš©íšŸìˆ˜': data_dict.get('R_CITATED_IDX', ''),
                'ë…¼ë¬¸ID': data_dict.get('R_SYST_LOCA_ID1', ''),
                'ì´ˆë¡': '',        
                'ì €ì_ìƒì„¸': '',   
                'í‚¤ì›Œë“œ_ìƒì„¸': '' 
            }
            temp_article_data['ì €ì'] = data_dict.get('R_CRET_NM', '')


            if article_url_path:
                full_detail_url = "https://www.kci.go.kr" + article_url_path
                print(f"    â†—ï¸ ìƒˆë¡œìš´ í˜ì´ì§€ì—ì„œ ìƒì„¸ í˜ì´ì§€ ì´ë™: '{temp_article_data['ì œëª©']}'")
                
                detail_page = await browser.new_page() 
                await detail_page.goto(full_detail_url, wait_until='domcontentloaded', timeout=60000) 
                await detail_page.wait_for_load_state('networkidle', timeout=60000) 
                await detail_page.wait_for_timeout(2000) 

                detail_abstract, detail_authors, detail_keywords = await extract_detail_info(detail_page)
                temp_article_data['ì´ˆë¡'] = detail_abstract
                temp_article_data['ì €ì_ìƒì„¸'] = detail_authors
                temp_article_data['í‚¤ì›Œë“œ_ìƒì„¸'] = detail_keywords
                print(f"    âœ… ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: ì €ì_ìƒì„¸='{detail_authors[:50]}...' í‚¤ì›Œë“œ='{detail_keywords[:50]}...'")

                await detail_page.close() 
                detail_page = None 
            
            articles_data.append(temp_article_data)
            print(f"  âœ… [{i+1}] '{temp_article_data.get('ì œëª©', '')}' ì¶”ì¶œ ì™„ë£Œ")

        except Exception as e:
            print(f"  âŒ [{i+1}] ë…¼ë¬¸ ì¶”ì¶œ ë˜ëŠ” ìƒì„¸ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ (ì˜¤ë¥˜: {e})")
            import traceback
            traceback.print_exc()
            if detail_page: 
                try:
                    await detail_page.close()
                except:
                    pass
            continue

async def run():
    async with async_playwright() as playwright:
        browser = await playwright.chromium.launch(headless=False)
        page = await browser.new_page() # ë©”ì¸ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€

        try:
            await page.goto(KCI_URL)
            await page.wait_for_load_state('load')
            await page.wait_for_timeout(1000)

            await page.fill('#topKeyword', f'{SEARCH_KEYWORD1} {SEARCH_KEYWORD2}')
            await page.click('button.searchbtn')
            await page.wait_for_load_state('networkidle')
            await page.wait_for_timeout(3000)

            for page_num in range(1, 11): 
                print(f"\n--- ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ {page_num}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘... ---")
                await page.wait_for_selector('table.search-answer-tbl > tbody > tr', timeout=40000)
                
                await extract_page_articles(page, browser)

                next_button = page.get_by_role("link", name="ï” ë‹¤ìŒí˜ì´ì§€") 
                
                if await next_button.count() == 0:
                    next_selector = f'a[href^="javascript:goPage({page_num + 1})"]'
                    next_button = page.locator(next_selector).filter(has_text=re.compile(r"^\d+$"))

                if await next_button.count() > 0:
                    if await next_button.is_enabled() and await next_button.is_visible():
                        print(f"    â¡ï¸ {page_num+1}í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
                        await next_button.click()
                        await page.wait_for_load_state('networkidle', timeout=40000)
                        await page.wait_for_timeout(2000)
                    else:
                        print("ğŸ”š ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì´ ë¹„í™œì„±í™”ë˜ì—ˆê±°ë‚˜ ìˆ¨ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤. ìŠ¤í¬ë˜í•‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                else:
                    print("ğŸ”š ë‹¤ìŒ í˜ì´ì§€ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë˜í•‘ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

            df = pd.DataFrame(articles_data)
            final_columns = [
                'ì œëª©', 'ì €ì_ìƒì„¸', 'ì´ˆë¡', 'í‚¤ì›Œë“œ_ìƒì„¸', 'ì €ì', 
                'ì €ë„ëª…', 'ë°œí–‰ê¸°ê´€', 'ê¶Œ', 'í˜¸', 'ì‹œì‘í˜ì´ì§€', 'ì¢…ë£Œí˜ì´ì§€',
                'ë°œí–‰ë…„ë„', 'ì£¼ì œë¶„ì•¼', 'ì¸ìš©íšŸìˆ˜', 'ë…¼ë¬¸ID'
            ]
            existing_cols = [col for col in final_columns if col in df.columns]
            df = df[existing_cols]

            df.to_csv('kci_articles_all_fields_with_details.csv', index=False, encoding='utf-8-sig')
            print(f"\nâœ… ì €ì¥ ì™„ë£Œ: kci_articles_all_fields_with_details.csv (ì´ {len(df)}ê±´)")

        except Exception as e:
            print(f"\nğŸš¨ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()

        finally:
            await browser.close()

if __name__ == '__main__':
    asyncio.run(run())